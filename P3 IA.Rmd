---
title: "P3 IA"
output: html_document
date: '2022-03-21'
---

Primer de tot descarreguem els paquets necessaris per a la realitzaci√≥ de la pr√†ctica:

```{r}
install.packages(c("ChemometricsWithR", "MASS"))
install.packages(c("e1071", "sfsmisc", "class", "caret"))
library("ChemometricsWithR")
library("MASS")
library("sfsmisc")
library("e1071")
library("class")
library("caret")
set.seed(43)
```

Loading the data:

```{r}
data("Prostate2000Raw", package="ChemometricsWithR") #anem utilitzar aquestes dades
mz_prost <- Prostate2000Raw$mz #creem tres variables, una amb cada informaci√≥ 
#mz √©s relaci√≥ carrega / massa
intensity_with_replicates <- Prostate2000Raw$intensity # intensitat de l'espectre
spectra_condition <- Prostate2000Raw$type #tipus de pacient que tenim
number_of_subjects <- ncol(intensity_with_replicates)/2 #el numero de pacients √©s el numero de columnes entre dos perqu√® tenim dues repliques de cadascun
subject_ids <- paste0("S", rep(1:number_of_subjects, each = 2)) 
# com que tenim dues r√®pliques diferents de cada pacient, voldrem que canvi√Ø cada dos, no a cada n√∫mero de manera que ens quedi: 1 1 2 2 3 3 4 4 ... 327 327
replicate_ids <- rep(c("A", "B"), times = number_of_subjects) 
#similar al pas anterior, volem classificar cada r√®plica d'un mateix pacient de manera que ens quedi: A B A B A B ... A B
new_names <- paste0(subject_ids,
                    "_",
                spectra_condition, # control control ... bph, bph,pca, pca
                   "_",
                replicate_ids) #ara agafem cadascun dels subjectes i els classifiquem segons la seva condici√≥ (control, c√†ncer o inflaci√≥ prost√†tica)
colnames(intensity_with_replicates) <- new_names #ara posem els noms que hem creat a cada columna
rownames(intensity_with_replicates) <- as.character(round(mz_prost, digits = 1)) #per la varibale de massa / c√†rrega la convertim a una string per que no es llegeixi com a un n√∫mero i ho arrodoneix a un digit
```

Preprocessing.

Replicate averaging: 

As each subject is measured twice, we will average consecutive spectra (belonging to the same subject):

```{r}
num_subjects <- ncol(intensity_with_replicates)/2 #el n√∫mero total de subjectes ser√† el n√∫mero total de columnes entre dos perqu√® tenim dues r√®pliques de cadascun
intensity_avg <- matrix(0, nrow = nrow(intensity_with_replicates), ncol = num_subjects) #creem una matriu buida amb tantes files com r√®pliques i tantes columnes com subjectes
for (i in seq(1, num_subjects)) {
intensity_avg[, i] <- rowMeans(intensity_with_replicates[, c(2*i - 1, 2*i)])
} #aqu√≠ estem la mitja dels valors de cada una de les files
```

spectra_condition has 654 values, one for each spectra. We take one every two types to have 327 values, one for each subject in our intensity_avg matrix:

```{r}
subject_condition <- spectra_condition[seq(from = 1, to = 654, by = 2)]
```

Log transformation:

Log transformation transforms the intensities to their log values. The measured intensities span a wide dynamic range of values. The same peak in some spectrum can be much larger than in other spectra. The distribution of intensities in the spectra is non-gaussian, and by using a log transform we can make it more similar to a gaussian distribution.
Having gaussian-like data is beneficial for PCA and LDA, as PCA is based on the covariance matrix and LDA assumes that our data is shaped in multivariate gaussians as well.
Therefore, if we create our models using the logarithm of the intensities instead of the intensities, we will be able to capture better the information of our largest peaks in the mass spectra, as their histogram will ressemble more a gaussian.
First we transform the intensity values to a log scale. To do that, we create a copy of our data and then transform it:

```{r}
intensity_log <- intensity_avg #creem una copia per no alterar l'original
```

Values close to zero would go to -infinity. We want to avoid that. A simple solution is to use a threshold:

```{r}
intensity_log[intensity_log < 1E-5] <- 1E-5 
intensity_log <- log(intensity_log) #els elements que comopleixin la condici√≥ de ser m√©s petits de 10^-5 els reajustarem a escala logar√≠tmica per tal que no generin problemes
```

If you look at a given m/z variable and check the intensity values you can see that our variables were not normally distributed: There are few values with large intensities, the distribution is not symmetric:

```{r}
hist(intensity_avg[2000,], breaks = 200, xlab = "Intensity (a.u.)", 
     main = sprintf("Histogram of 327 raw intensities with m/z = %f Da",
mz_prost[2000])) #estem mirant el conjunt d'intensitats dels diferents pacients al punt 2000
```

Podem veure com la distribuci√≥ de l'histograma no √©s gaussiana de manera que l'ajustem logar√≠tmicament i ara ens surt el seg√ºent:

```{r}
hist(intensity_log[2000,], breaks = 200, xlab = "log-Intensity (a.u.)", main = sprintf("Histogram of 327 log intensities with m/z = %f Da",
mz_prost[2000]))
```
Ara veiem que la distribuci√≥ √©s molt m√©s gaussiana; sim√®trica etc.

The classification algorithms assume that each sample is given in a row, therefore we need to transpose the intensity matrix:

```{r}
intensity <- t(intensity_log) #creem una nova variable que sigui igual a la trasposada
```
Let‚Äôs just check the dimensionality to confirm:
```{r}
message("Number of samples: ", nrow(intensity)) ## Number of samples: 327 (pacient que tenim)
message("Number of variables: ", ncol(intensity)) ## Number of variables: 10523 (n√∫mero de punts del cromatograma)
```

The balance of sample types in the dataset is important to many algorithms: If we had very few samples of a particular class (for instance very few benign prostatic hyperplasia subjects), we would have to consider either (i) looking for more samples of that class, (ii) drop all the hyperplasia samples and simplify the experiment or (iii) use algorithms able to work with unbalanced datasets.

```{r}
table(subject_condition) #tindr√† la seg√ºent forma
## subject_condition
##     bph control     pca
##      78      81     168
```
Q1. Is the dataset balanced? What is the percentage of samples of each class?

No est√† balanced perqu√® el nombre de subjectes per a cada condici√≥ no √©s igual, sin√≥ que els percentatges s√≥n els seg√ºents:

- bph: 23'86%
- pca: 51'38%
- control: 24'7%

Per tal que fos balanced, hauriem de tenir un 33'33% per a cadascun de les classes.

---

As you may see the dimensionality of the raw data is pretty high. The usual procedure to reduce this type of data consist of finding common peaks and integrating their area (aside from smoothing, binning, peak alignment, normalization and other signal processing steps to enhance signal quality).
However, here to simplify we will follow a brute force strategy (not optimal but easy): We will consider every single point in the spectra as a distinctive feature. This brute force strategy is sometimes used in bioinformatics, but generally does not provide the best results.

---

Train / Test division

In order to estimate how our trained model will perform, we need to split the dataset into a training subset and a test subset. The train subset will be used to train the model and the test subset will be used to estimate the performance of the model.
Some classifiers perform better with balanced datasets. We will balance the dataset so it has the same number of samples for each class in the train subset.
The number of samples used in the train subset for each class corresponds to 70% of the number of samples of the least populated class.

```{r}
num_samples_per_class_in_train <- round(0.7*min(table(subject_condition))) #aqu√≠ estem agafant el 70% de la taula amb la classe que t√© menys, √©s a dir, de bph
pca_idx <- which(subject_condition == "pca") #ens dona els √≠ndexs dels subjectes que t√© pca
pca_idx_train <- sample(pca_idx, num_samples_per_class_in_train) #nova variable que emmagatzema de forma aleat√≤ria uns 56 pacients que √©s un 70% de 78% dels pacients que tenien pca
#√©s a dir, 56 pacients van al train i la resta aniran al test
pca_idx_test <- setdiff(pca_idx, pca_idx_train) #fa la difer√®ncia entre els subjectes del train i el total dels que tenen pca, √©s a dir, els "compara" i agafa els que no s'han utilitzat per fer el train
#ara veiem com fa exactament el mateix per√≤ per a la resta condicions
bph_idx <- which(subject_condition == "bph")
bph_idx_train <- sample(bph_idx, num_samples_per_class_in_train) 
bph_idx_test <- setdiff(bph_idx, bph_idx_train)
ctrl_idx <- which(subject_condition == "control")
ctrl_idx_train <- sample(ctrl_idx, num_samples_per_class_in_train) 
ctrl_idx_test <- setdiff(ctrl_idx, ctrl_idx_train)
train_idx <- c(pca_idx_train, bph_idx_train, ctrl_idx_train) 
test_idx <- c(pca_idx_test, bph_idx_test, ctrl_idx_test)
# use the indexes to split the matrix into train and test
# fem dues matrius diferents, una de train i una de test
# separem tamb√© els valors de les intensitats en train i test
intensity_trn <- intensity[train_idx,] 
intensity_tst <- intensity[test_idx,]
# use the indexes to split the labels
# fem el mateix per√≤ assignant les condicions de cada subjecte
subject_condition_trn <- subject_condition[train_idx] 
subject_condition_tst <- subject_condition[test_idx]
message("Number of samples in training: ", nrow(intensity_trn))
## Number of samples in training: 165
message("Number of samples in test: ", nrow(intensity_tst)) 
## Number of samples in test: 162
```

Feature Extraction by Principal Component Analysis

---

In most occasions, classifiers are not built in the raw data due to theexistence of a large number of dimensions without discriminant information.
In this section we will explore Dimensionality Reduction by Principal Component Analysis (PCA). PCA is an unsupervised technique that returns directions (eigenvectors) that explain the maximum data variance while being orthogonal among them. While PCA is optimal in compressing the data into a lower dimensionality, we have to be aware that maximum variance does not imply maximum discriminability. In other words, PCA may not be the optimal procedure to reduce the dimensionality. It is however, the default technique for initial data exploration and pattern recognition design.

ùê∑=ùëÜ‚àóùêø +ùê∏

PCA will find a new basis for our data in which each of the directions maximize the explained variance of our data. The change of basis matrix ùêø is called the PCA loadings, while the projection of our data in this new basis are the scores.

If we truncate the PCA decomposition to a number of principal components npc, then the product 
ùëÜ‚àóùêøùëá will not be exactly as ùê∑, and the difference will be the matrix of residuals ùê∏.
We will use the PCA function from the ChemometricsWithR package. PCA needs to be applied to a matrix where each feature has zero mean.

---

A continuaci√≥ creem una matriu de zeros i l'anem omplint amb les variances corresponents a cada subjecte:

```{r}
mean_spectrum_trn <- colMeans(intensity_trn)
intensity_trn_preproc <- matrix(0, nrow = nrow(intensity_trn), ncol = ncol(intensity_trn))
for (i in 1:ncol(intensity_trn_preproc)) {
intensity_trn_preproc[, i] <- intensity_trn[, i] - mean_spectrum_trn[i] }
```

The test data is centered using the mean and standard deviation computed from the train data:

Ara estem fent el mateix per√≤ agafant les dades dels subjectes que hem assignat com a test:

```{r}
intensity_tst_preproc <- matrix(0, nrow = nrow(intensity_tst), ncol = ncol(intensity_tst))
for (i in 1:ncol(intensity_tst_preproc)) {
intensity_tst_preproc[, i] <- intensity_tst[, i] - mean_spectrum_trn[i] }
```

We can now perform the dimensionality reduction and observe how the variance is distributed among the first principal components:

Un cop tenim les dades ordenades podem procedir a fer el PCA amb la funci√≥ ia continuaci√≥ representar aquests resultats m√©s visualment a trav√©s d'un gr√†fic on posarem mostrarem les variances calculades en funci√≥ del n√∫mero de components que combinem:

```{r}
pca_model <- PCA(intensity_trn_preproc) 
summary(pca_model)
pca_model_var <- variances(pca_model)
pca_model_var_percent <- 100*cumsum(pca_model_var)/sum(pca_model_var)
plot(x = 1:length(pca_model_var_percent),
y = pca_model_var_percent,
type = "l",
xlab = "Number of principal components", ylab = "Cummulated variance (%)",
main = "% variance captured vs #PC used")
```
Based on the knee of the plot, we choose the number of components of the PCA space. This is a very rough way to choose the number of components of the PCA space. Usually this is done with a cross-validation, as you will see at last section of the session.

A partir del gr√†fic podem veure que el valor que correspon a un 70% de varian√ßa √©s al voltant dels 50 components (eix de les x), de manera que creem una nova variable que correspongui a aquest n√∫mero:

```{r}
pca_model_ncomp <- 50
```

We can retrieve the scores of the PCA model:
The scores of the training subset of the model can be obtained using the function scores.

Creem una variable que emmagatzemi els resultats del PCA que hem fet abans pels models assignats al training:

```{r}
intensity_scores_trn <- pca_model$scores[,1:pca_model_ncomp]
```

If we want to obtain the scores for our test subset we need to project our samples into the PCA space represented in our model:

Ara fem el mateix per√≤ pels subjectes assignats al test:

```{r}
intensity_scores_test <- project(pca_model, npc = pca_model_ncomp, intensity_tst_preproc)
```

Each row in the scores matrices contains the projection of a single sample. Each column in the scores matrices represents a principal component. Each value corresponds to the projection of the sample given by its row on the principal component given by its column.
To visualize the scores we can plot some principal components in pairs:

Ara volem veure gr√†ficament aquest conjunt de resultats i el que farem √©s asisgnar a cada par√†metres un color diferent i visualitzar-los per a conjunts diferents de dimensions comparant el resultats del train i el del test:

```{r}
colours_legend <- c(bph = "blue", control = "darkgreen", pca = "red") 
point_shapes_legend <- c(train = 1, test = 4) # 1 means circle, 4 means cross (see ?par)

intensity_scores_all <- rbind(intensity_scores_trn, intensity_scores_test)

point_shapes <- point_shapes_legend[c(rep("train", nrow(intensity_scores_trn)),
rep("test",nrow(intensity_scores_test)))]

colours <- colours_legend[c(subject_condition_trn, subject_condition_tst)]

pairs(intensity_scores_all[,1:5], labels = paste("PC", 1:5), pch = point_shapes, col = colours)
```

Q2. Is any of these first 5 principal components able to discriminate sample types?

A partir del gr√†fic podem veure que les dimensions que millor discriminen els diferents grups serien PC3 i PC4 ja que podem entreveure millor els diferents grups.

We can plot any of those subplots as well:

Veiem que podeu veure cadacsun dels gr√†fics individualment de manera que els podrem interpretar m√©s f√†cilment:

```{r}
plot(x = intensity_scores_all[,1],
     y = intensity_scores_all[,2],
     pch = point_shapes,
     col = colours,
main = "PCA Score plot", xlab = "PC 1",
ylab = "PC 2")
legend("topright",
legend = names(colours_legend), pch = 19,
title = "Condition",
col = colours_legend)
legend("topleft",
legend = names(point_shapes_legend), pch = point_shapes_legend,
title = "Subset",
col = "black")
```

Q3. Do you see if the classes differ a lot from being normally distributed?

Si mirem els gr√†fics podem veure que realment no hi ha cap combinaci√≥ de dimensions que ens permeti discrimar realment les diferents classes. Basant-nos en aix√≤ podr√≠em dir que no es possible aplicar un "Linear Discriminant Analysis" que ens serveixi com a model per identificar les diferents categories.

Nearest Neighbours Classifier on the PCA space:

A nearest neighbour classifier is a non-parametric method used for pattern recognition and classification. The method classifies new samples based on the classes of the k closest training samples.
The k-NN classifier is implemented in the class package.
The kNN will predict the classes of the test set, given the training samples, their
classes and the number of neighbours to look.
We will apply the k-NN classifier on the PCA space.

```{r}
library("class")
library("pls")
```
Q4. We can calculate the confusion matrix. Take some time to understand this table. What groups of samples is the kNN misclassifying?

```{r}
subject_condition_tst_pca_knn_pred <- class::knn(train = intensity_scores_trn, test = intensity_scores_test, cl = subject_condition_trn, k= 5)

confmat_pca_knn <- table(subject_condition_tst, subject_condition_tst_pca_knn_pred) 
print(confmat_pca_knn)

CR_pca_knn <- sum(diag(confmat_pca_knn))/sum(confmat_pca_knn) 
message("The classification rate for PCA-kNN is: ", 100*round(CR_pca_knn, 2), "%")
```

Amb aquet tros de codi hem creat una taula per veure m√©s clarament la comparaci√≥ entre els resultats del nostre algoritme i les categories de veritat, √©s a dir, si comparem files i columnes podem veure els subjectes que hem "missclassified". En aquest cas, podem veure que el grup amb m√©s error seria els de la categoria pca ja que podem veure que hi ha 51 que el programa preveu que seran bph per√≤ que realment son pca.

Linear Discriminant Analysis Classifiers:

Linear Discriminant Analysis Classifiers assume classes are normally distributed. Here we will apply them and we will check the classification performance. They cannot be implemented in the original space due to insufficient number of samples leading to singular covariance matrices, however we can apply Linear Discriminant Analysis on the PCA scores.
LDA assumes the groups (or classes) behave like multivariate gaussians. It will find the change of basis that maximizes the between-class distance while minimizing the within-class distance. In other words, it tries to find the change of basis that separates our clusters while making our clusters as compact as possible.

In this section we will build a linear classifier using the principal components of the data by implementing the next steps:

```{r}
library(MASS)

#Train a LDA model using the training scores obtained from the previous PCA, you can use function lda from the MASS package.
#Use the function predict to make the prediction of the test samples.
model = lda(intensity_scores_trn, grouping = subject_condition_trn)
model

prediction = predict(model, intensity_scores_test)
prediction

#From the predicted object extract: a) the projection of the samples in the LDA space and b) the predicted class

prediction_class = prediction$class
prediction_class

#Compute the confusion Matrix and estimate the classification Rate

matriu_confusio = table(subject_condition_tst, prediction_class)
matriu_confusio

rate = sum(diag(matriu_confusio))/sum(matriu_confusio) 
rate*100

# Make a plot of the data using two dimensions. To visualize both the training and test data we will compute the projections for the training data as well.

total_lda = lda(intensity_scores_all, grouping(subject_condition))
total_lda

valors_lda = predict(total_lda)
plot(valors_lda$x[,1], valors_lda$x[,2])
text(valors_lda$x[,1], valors_lda$x[,2], subject_condition, cex = 0.7, pos = 4, col = 'blue')
plot(x = valors_lda$x[,1], y = valors_lda$x[,2], pch = point_shapes, col = colours, main = 'Resultats LDA', xlab = 'LDA1', ylab = 'LDA2')
legend('topright', legend = names(colours_legend), pch = 17, title = 'Condici√≥', col = colours_legend)

```

Questions:

Q5. What would have happened if the researchers had not considered including benign prostatic hypertrophy subjects in the experimental design?

El que segurament hagu√©s passat √©s que pacients amb c√†ncer, probablement en fases inicials de la malaltia, es classificarien com a pacients amb hipertr√≤fia, ja que el model no tindria suficient precisi√≥ com per clasificar-lo com a malalt de c√†ncer. De la mateixa manera, pacients amb una hipertr√≤fia benigne considerable es podrien "missclassify" com a pacients amb c√†ncer degut a que, com hem dit, el nostre model no estaria entrenat per a aquest tipus de dades.

Q6. What would our prediction be if we built a model without benign prostatic hypertrophy samples and later on we tried to predict a patient who suffered from that condition?

En aquest cas, donat que el nostre model nom√©s classifica en dues categories diferents que s√≥n el c√†ncer de control i el c√†ncer de pr√≤stata, subjectes amb hipertr√≤fia prost√†tica benigna tendiran a ser categoritzats al grup amb prost√†tic c√†ncer de forma err√≤nia en el cas de tenir alguns par√†metres alterats, o b√©, a pacients normals tot i tenir una hiperpl√†sia benigne.

Veiem com en ambdos casos el resultat ser√† un algortime erroni per al diagn√≤stic d'aquests tipus de patologies, de manera que acabem amb la conclusi√≥ que √©s de vital import√†ncia tant incloure les samples d'hipertr√≤fia benigne a la part experimental, com considerar-la com a categoria a l'hora de classificar els pacients.



